{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specific extraction of information from different formats\n",
    "\n",
    "# This extracts the latency in milliseconds from the ping's result format\n",
    "# 64 bytes from 13.66.225.134: icmp_seq=1 ttl=47 time=36.2 ms => 36.2\n",
    "def get_ping_list(file_name):\n",
    "    l = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                l.append(float(line.split()[6].split(\"=\")[1]))\n",
    "            except:\n",
    "                continue\n",
    "        if len(l) < 1000:\n",
    "            print file_name # to catch any deficiencies\n",
    "        return l\n",
    " \n",
    "# This extracts the latency from files that just have a number per row\n",
    "def get_integer_list(file_name):\n",
    "    l = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                l.append(int(line))\n",
    "            except:\n",
    "                continue\n",
    "    return l\n",
    "\n",
    "\n",
    "# This extracts all latency breakdown results from the giza_latency_raw directory\n",
    "# returns a map {key, [list of latency for the key]}\n",
    "def get_giza_latency_raw_dir_map(file_name):\n",
    "    from collections import defaultdict\n",
    "    d = defaultdict(list)\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            if len(l) == 3:\n",
    "                key = l[0] + l[1]\n",
    "                d[key].append(int(l[2]))\n",
    "            elif len(l) == 4:\n",
    "                key = l[0] + l[1] + l[2]\n",
    "                d[key].append(int(l[3]))\n",
    "            else:\n",
    "                if len(l) == 0 or len(l) == 1:\n",
    "                    continue\n",
    "\n",
    "                d[l[0]].append(int(l[1]))\n",
    "    return d\n",
    "\n",
    "# helper mapping for data extraction within giza_latency_raw directory\n",
    "def get_dc_ip_mappings():\n",
    "    dic = {}\n",
    "    dic['south-central'] = '13.65.92.139'\n",
    "    dic['west1'] = \"13.93.236.162\"\n",
    "    dic['central'] = \"localhost\"\n",
    "    dic['west-central'] = '52.161.28.134'\n",
    "    dic['north-central'] = '157.56.29.194'\n",
    "    dic['east1'] = '191.237.41.69'\n",
    "    dic['east2'] = '13.68.110.92'\n",
    "    dic['us-south-central'] = '13.65.92.139'\n",
    "    dic['us-west1'] = \"13.93.236.162\"\n",
    "    dic['us-central'] = \"localhost\"\n",
    "    dic['asia-japan-east'] = \"13.78.83.9\"\n",
    "    dic['eu-north-europe'] = '52.178.201.184'\n",
    "    dic['eu-uk-west'] = '51.141.11.143'\n",
    "    dic['asia-japan-west'] = '104.214.146.200'\n",
    "    \n",
    "    #     dic['japan-east'] = \"13.78.83.9\"\n",
    "#     dic['north-europe'] = '52.178.201.184'\n",
    "    return dic\n",
    "\n",
    "# helper function to get the configuration information\n",
    "def get_configurations(configuration):\n",
    "    configuration_names = ['2-1-us', '2-1-world', '6-1-us', '6-1-world']\n",
    "    configurations = [[\"central\", \"south-central\", \"west1\"],\n",
    "                      [\"us-central\", \"eu-north-europe\", \"asia-japan-east\"],\n",
    "                      [\"central\", \"west-central\", \"south-central\",\n",
    "                       \"north-central\", \"east1\", \"east2\", \"west1\"],\n",
    "                      [\"us-central\", \"us-south-central\", \"us-west1\",\n",
    "                       \"eu-north-europe\", \"eu-uk-west\", \"asia-japan-east\",\n",
    "                       \"asia-japan-west\"]]\n",
    "    return (configuration_names[configuration], configurations[configuration])\n",
    "\n",
    "\n",
    "# This is used to extract the specific thrift latency from the dc to all other dc's storage within a giza experiment\n",
    "# results are limited to:\n",
    "#   - central to [central, west1, south central]\n",
    "#   - central to [central, japan east, north europe]\n",
    "#   - central to [centra, west central, north central, south central, east1, east2, west1]\n",
    "#   - central to [central, west1, south central, japan east, japan west, north europe, uk west]\n",
    "# input:\n",
    "#    configuration :\n",
    "#      0: us-2-1\n",
    "#      1: world-2-1\n",
    "#      2: us-6-1\n",
    "#      3: world-6-1\n",
    "#    size: 256kb, 1mb, 4mb\n",
    "#    op: put, get\n",
    "def get_thrift_storage_lists(configuration, size, op):\n",
    "    # mapping info\n",
    "    dic = get_dc_ip_mappings()\n",
    "    configuration_name, configurations = get_configurations(configuration)\n",
    "    #configuration_names = ['2-1-us', '2-1-world', '6-1-us', '6-1-world']\n",
    "    #configurations = [[\"central\", \"south-central\", \"west1\"],\n",
    "#                       [\"us-central\", \"eu-north-europe\", \"asia-japan-east\"],\n",
    "#                       [\"central\", \"west-central\", \"south-central\",\n",
    "#                        \"north-central\", \"east1\", \"east2\", \"west1\"],\n",
    "#                       [\"us-central\", \"us-south-central\", \"us-west1\",\n",
    "#                        \"eu-north-europe\", \"eu-uk-west\", \"asia-japan-east\",\n",
    "#                        \"asia-japan-west\"]]\n",
    "    if op == 'get':\n",
    "        thrift_op = \"GET_Thrift_Storage_\"\n",
    "        azure_op = \"Get_Azure_\"\n",
    "    \n",
    "    else:\n",
    "        thrift_op = \"Put_Thrift_Storage_\"\n",
    "        azure_op = \"Put_Azure_\"\n",
    "    thrift_file_name = '/'.join(['giza_latency_raw', configuration_name, size, op, 'giza_trace.log'])\n",
    "    thrift_file_map = get_giza_latency_raw_dir_map(thrift_file_name)\n",
    "    thrift_list = []\n",
    "    azure_list = []\n",
    "    for dc in configurations:\n",
    "        azure_file_name = dc + '_storage_server.log'\n",
    "        azure_file_name = '/'.join(['giza_latency_raw', configuration_name, size, op, azure_file_name])\n",
    "        azure_file_map = get_giza_latency_raw_dir_map(azure_file_name)\n",
    "        if configuration == 0 or configuration == 2:\n",
    "            azure_list.append(azure_file_map[azure_op + 'us-' +  dc])\n",
    "        else:\n",
    "            azure_list.append(azure_file_map[azure_op + dc])\n",
    "        thrift_list.append(thrift_file_map[thrift_op + dic[dc]])\n",
    "    return (thrift_list, azure_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specifically for Data Analysis\n",
    "\n",
    "# get 2d arrar (matrix graph) of the different dc access. Each element is a list of result that can \n",
    "# be summarized\n",
    "# returns a map where key is a tuple\n",
    "def get_2d_data(from_dcs, to_dcs, file_format):\n",
    "    d = {}\n",
    "    for dc1 in from_dcs:\n",
    "        for dc2 in to_dcs:\n",
    "            d[(dc1, dc2)] = get_ping_list(file_format.format(dc1, dc2))\n",
    "    return d\n",
    "\n",
    "# reducer of the results returned by get_2d_data\n",
    "# can be map or vector\n",
    "def do_2d_analysis(data, data_points, reducer, *args):\n",
    "    dict\n",
    "    if type(data) is dict:\n",
    "        d = {}\n",
    "        for key in data:\n",
    "            if len(data[key]) == data_points:\n",
    "                d[key] = reducer(data[key], *args)\n",
    "            else:\n",
    "                d[key] = -1\n",
    "        return d\n",
    "    if type(data) is list:\n",
    "        d = []\n",
    "        for datum in data:\n",
    "            d.append(reducer(datum))\n",
    "        return d\n",
    "\n",
    "# convert any analysis data into a table\n",
    "def make_2d_table(from_dcs, to_dcs, data, file_name):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    table = np.zeros((len(from_dcs), len(to_dcs)), dtype=np.int)\n",
    "    for i in range(len(from_dcs)):\n",
    "        for j in range(len(to_dcs)):\n",
    "            table[i][j] = data[(from_dcs[i], to_dcs[j])]\n",
    "    table = pd.DataFrame(table, index=from_dcs, columns=to_dcs)\n",
    "    table.to_csv(file_name, index=True, header=True, sep=',')\n",
    "\n",
    "# given data points, calculate different percentiles\n",
    "def get_percentiles(data, percentiles):\n",
    "    l = [] \n",
    "    for p in percentiles:\n",
    "        import numpy as np\n",
    "        l.append(np.percentile(data, p))\n",
    "    return l\n",
    "\n",
    "# create bargraph and allows stacking with extra data\n",
    "# input has to be in the form of [[raw_data1], [raw_data2]. etc]\n",
    "def bar_graph_with_error(data1, data2, data_names, x_names, y_label, fname, y_range, no_mean = True):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    f = plt.figure()\n",
    "    ind = np.arange(len(data1))\n",
    "    width = 0.25\n",
    "    if no_mean:\n",
    "        data1_mean = [np.mean(x)/1000 for x in data1]\n",
    "        data2_mean = [np.mean(x)/1000 for x in data2]\n",
    "    \n",
    "#     data1_err = [np.std(x)/1000 for x in data1]    \n",
    "#     data2_err = [np.std(x)/1000 for x in data2]\n",
    "    p1 = plt.bar(ind, data1_mean, width, color='r') #, yerr=data1_err)\n",
    "    if data2 != '':\n",
    "        p2 = plt.bar(ind, data2_mean, width, color='y')#, bottom = data1_mean) #, yerr=data2_err)\n",
    "        plt.legend((p1[0], p2[0]), data_names)\n",
    "    plt.xticks(ind + width/2., x_names)\n",
    "    plt.yticks(np.arange(0, y_range, 50))\n",
    "    plt.ylabel(y_label)\n",
    "#     plt.legend((p1[0], p2[0]), data_names)\n",
    "#     plt.show()\n",
    "    pp = PdfPages(fname + '.pdf')\n",
    "    pp.savefig(f)\n",
    "    plt.clf()\n",
    "    pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ping_latency_raw/west-central-to-north-europe.log\n",
      "ping_latency_raw/north-europe-to-west-central.log\n"
     ]
    }
   ],
   "source": [
    "# Get the ping latencies\n",
    "import numpy as np\n",
    "dcs = ['central', 'west-central', 'north-central', 'east1', 'east2', 'south-central', 'west1', 'west2', 'north-europe', 'uk-west', 'japan-east', 'japan-west']\n",
    "file_format = 'ping_latency_raw/{}-to-{}.log'\n",
    "data = get_2d_data(dcs, dcs, file_format)\n",
    "percentiles = [10, 50, 75, 95]\n",
    "for p in percentiles:\n",
    "    data_summary = do_2d_analysis(data, 1000, np.percentile, p)\n",
    "    make_2d_table(dcs, dcs, data_summary, 'ping_latency_readable/ping_' + str(p) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the tunneling latency\n",
    "import numpy as np\n",
    "from_dcs = ['central']\n",
    "configurations = [0, 2]\n",
    "sizes = ['256kb', '1mb', '4mb']\n",
    "ops = ['put', 'get']\n",
    "cur_dir = 'azure_storage_latency_graph/tunneling'\n",
    "for configuration in configurations:\n",
    "    for size in sizes:\n",
    "        for op in ops:\n",
    "            config, to_dcs = get_configurations(configuration)\n",
    "            thrift_latency, azure_latency = get_thrift_storage_lists(configuration, size, op)\n",
    "#             transfer_latency = []\n",
    "#             for i in range(len(thrift_latency)):\n",
    "#                 transfer_latency.append(np.subtract(thrift_latency[i], azure_latency[i]))\n",
    "            bar_graph_with_error(thrift_latency, azure_latency, ('Transfer', 'Azure'), to_dcs, 'Latency (ms)', '{}/{}-{}-{}'.format(cur_dir, config, op, size), 301)\n",
    "configurations = [1, 3]\n",
    "sizes = ['256kb', '1mb', '4mb']\n",
    "ops = ['put', 'get']\n",
    "cur_dir = 'azure_storage_latency_graph/tunneling'\n",
    "for configuration in configurations:\n",
    "    for size in sizes:\n",
    "        for op in ops:\n",
    "            config, to_dcs = get_configurations(configuration)\n",
    "            thrift_latency, azure_latency = get_thrift_storage_lists(configuration, size, op)\n",
    "#             transfer_latency = []\n",
    "#             for i in range(len(thrift_latency)):\n",
    "#                 transfer_latency.append(np.subtract(thrift_latency[i], azure_latency[i]))\n",
    "            bar_graph_with_error(thrift_latency, azure_latency, ('Transfer', 'Azure'), to_dcs, 'Latency (ms)', '{}/{}-{}-{}'.format(cur_dir, config, op, size), 401)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = 128\n",
    "cur_dir = 'azure_storage_latency_graph/no_tunneling'\n",
    "to_dc = ['central', 'south-central', 'west1']\n",
    "config, to_dcs = get_configurations(0)\n",
    "from_dc = 'central'\n",
    "storage_latency = []\n",
    "file_format = 'azure_storage_latency_raw/{}/azure_storage_get_{}kb.txt_{}_to_{}.log'\n",
    "for dc in to_dc:\n",
    "    storage_latency.append(get_giza_latency_raw_dir_map(file_format.format(from_dc, size, from_dc, dc))['main'])\n",
    "bar_graph_with_error(storage_latency, '', '', to_dc, 'Latency (ms)', '{}/{}-{}-{}'.format(cur_dir, config, 'put', '256kb'), 301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "lf = []\n",
    "lt = []\n",
    "with open ('azure_storage_latency_raw/central/blob_ss_h.log', 'r') as f:\n",
    "    for line in f:\n",
    "        lf.append(line)\n",
    "with open ('azure_storage_latency_raw/central/blob_ss_t.log', 'r') as f:\n",
    "    for line in f:\n",
    "        lt.append(line)\n",
    "lf.sort()\n",
    "lt.sort() \n",
    "current_string = \"\"\n",
    "max_counter = 0\n",
    "min_counter = 0\n",
    "storages = ['blob.by4prdstr03a.store.core.windows.net.http',\n",
    "           'blob.dm5prdstr02a.store.core.windows.net.http',\n",
    "           'blob.sn4prdstr03a.store.core.windows.net.http',\n",
    "           'blob.cy4prdstr01a.store.core.windows.net.http',\n",
    "           'blob.db4prdstr01a.store.core.windows.net.http']\n",
    "s_map = {\n",
    "    'blob.by4prdstr03a.store.core.windows.net.http': 'west1',\n",
    "    'blob.dm5prdstr02a.store.core.windows.net.http': 'central',\n",
    "    'blob.sn4prdstr03a.store.core.windows.net.http': 'south-central',\n",
    "    'blob.cy4prdstr01a.store.core.windows.net.http': 'japan-east',\n",
    "    'blob.db4prdstr01a.store.core.windows.net.http': 'north-europe'\n",
    "}\n",
    "d = defaultdict(list)\n",
    "ed = defaultdict(list)\n",
    "for i in range(len(lf)):\n",
    "    lfs = lf[i].split()\n",
    "    lfs[4] = lfs[4][:-1]\n",
    "    lts = lt[i].split()\n",
    "    before = 0\n",
    "    after = 0\n",
    "    if lfs[4] in storages and lts[2] in storages:\n",
    "        before = datetime.strptime(lfs[0], \"%H:%M:%S.%f\")\n",
    "        after = datetime.strptime(lts[0], \"%H:%M:%S.%f\")\n",
    "        if current_string == \"\":\n",
    "            current_string = lfs[4]\n",
    "            d[current_string + str(max_counter)].append(after-before)\n",
    "            ed[s_map[current_string] + str(max_counter)].append(after-before)\n",
    "        else:\n",
    "            if current_string != lfs[4]:\n",
    "                if min_counter == 4:\n",
    "                    min_counter = 0\n",
    "                    max_counter += 1\n",
    "                else:\n",
    "                    min_counter += 1\n",
    "                current_string = lfs[4]\n",
    "                d[current_string + str(max_counter)].append(after-before)\n",
    "                ed[s_map[current_string] + str(max_counter)].append(after-before)\n",
    "            else:\n",
    "                d[current_string + str(max_counter)].append(after-before)\n",
    "                ed[s_map[current_string] + str(max_counter)].append(after-before)\n",
    "    else:\n",
    "        if lfs[4] != lts[2] and lts[2][-1] != 's' and lfs[4][-1] != 's':\n",
    "            print 'error: ' + lfs[4] + '  ' + lts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['north-europe1',\n",
       " 'north-europe0',\n",
       " 'north-europe3',\n",
       " 'north-europe2',\n",
       " 'north-europe5',\n",
       " 'north-europe4',\n",
       " 'central5',\n",
       " 'west14',\n",
       " 'central0',\n",
       " 'central3',\n",
       " 'central2',\n",
       " 'west10',\n",
       " 'central4',\n",
       " 'west12',\n",
       " 'west13',\n",
       " 'central1',\n",
       " 'west15',\n",
       " 'south-central3',\n",
       " 'south-central2',\n",
       " 'south-central1',\n",
       " 'south-central0',\n",
       " 'south-central5',\n",
       " 'south-central4',\n",
       " 'japan-east5',\n",
       " 'japan-east4',\n",
       " 'japan-east2',\n",
       " 'japan-east1',\n",
       " 'japan-east0',\n",
       " 'japan-east3',\n",
       " 'west11']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 3 are put and later 3 are get\n",
    "for k in ed:\n",
    "    ed[k] = [x.microseconds for x in ed[k]]\n",
    "ed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transfer_latency = [np.mean(ed['central3']), np.mean(ed['south-central3']), np.mean(ed['west13'])]\n",
    "size = 128\n",
    "cur_dir = 'azure_storage_latency_graph/no_tunneling'\n",
    "to_dc = ['central', 'south-central', 'west1']\n",
    "config, to_dcs = get_configurations(0)\n",
    "from_dc = 'central'\n",
    "storage_latency = []\n",
    "file_format = 'azure_storage_latency_raw/{}/azure_storage_get_{}kb.txt_{}_to_{}.log'\n",
    "for dc in to_dc:\n",
    "    storage_latency.append(get_giza_latency_raw_dir_map(file_format.format(from_dc, size, from_dc, dc))['main'])\n",
    "\n",
    "azure_latency = []\n",
    "for i in range(len(storage_latency)):\n",
    "    azure_latency.append(np.mean(storage_latency) - transfer_latency[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar_graph_with_error(storage_latency, azure_latency, ('Transfer', 'Azure'), to_dc, 'Latency (ms)', '{}/{}-{}-{}'.format(cur_dir, config, 'put', '256kb'), 301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'microseconds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-c835e4704e9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwest1put\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmicroseconds\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'west10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwest1put\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcur_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'azure_storage_latency_graph/no_tunneling'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mto_dc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'central'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'south-central'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'west1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'microseconds'"
     ]
    }
   ],
   "source": [
    "west1put = [x.microseconds for x in ed['west10']]\n",
    "np.mean(west1put)\n",
    "size = 128\n",
    "cur_dir = 'azure_storage_latency_graph/no_tunneling'\n",
    "to_dc = ['central', 'south-central', 'west1']\n",
    "config, to_dcs = get_configurations(0)\n",
    "from_dc = 'central'\n",
    "storage_latency = []\n",
    "file_format = 'azure_storage_latency_raw/{}/azure_storage_get_{}kb.txt_{}_to_{}.log'\n",
    "for dc in to_dc:\n",
    "    storage_latency.append(get_giza_latency_raw_dir_map(file_format.format(from_dc, size, from_dc, dc))['main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19:39:02.010536',\n",
       " 'IP',\n",
       " 'blob.by4prdstr03a.store.core.windows.net.http',\n",
       " '>',\n",
       " '10.0.0.5.49750:',\n",
       " 'Flags',\n",
       " '[S.],',\n",
       " 'seq',\n",
       " '909808302,',\n",
       " 'ack',\n",
       " '2022989648,',\n",
       " 'win',\n",
       " '8192,',\n",
       " 'options',\n",
       " '[mss',\n",
       " '1440,nop,wscale',\n",
       " '8,sackOK,TS',\n",
       " 'val',\n",
       " '253666243',\n",
       " 'ecr',\n",
       " '20875896],',\n",
       " 'length',\n",
       " '0']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blob.by4prdstr03a.store.core.windows.net.http:\n",
      "['19:39:02.010536 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49750: Flags [S.], seq 909808302, ack 2022989648, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 253666243 ecr 20875896], length 0\\n', '19:39:02.449507 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49751: Flags [S.], seq 551815351, ack 1238777423, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 1786257 ecr 20876005], length 0\\n', '19:39:02.888681 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49752: Flags [S.], seq 3044760159, ack 404026492, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 255288053 ecr 20876115], length 0\\n', '19:39:03.343625 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49753: Flags [S.], seq 60333383, ack 1667997207, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 251467366 ecr 20876229], length 0\\n', '19:39:03.809915 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49755: Flags [S.], seq 202611843, ack 3548123325, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 248512557 ecr 20876345], length 0\\n', '19:39:04.246978 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49758: Flags [S.], seq 2086348540, ack 2837637331, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 248509474 ecr 20876455], length 0\\n', '19:39:04.679987 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49760: Flags [S.], seq 3054937189, ack 3104547663, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 255282684 ecr 20876563], length 0\\n', '19:39:05.119058 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49761: Flags [S.], seq 1496757476, ack 3145758879, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 249361670 ecr 20876673], length 0\\n', '19:39:05.562016 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49762: Flags [S.], seq 3685096597, ack 3633931970, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 254404966 ecr 20876784], length 0\\n', '19:39:06.000112 IP blob.by4prdstr03a.store.core.windows.net.http > 10.0.0.5.49763: Flags [S.], seq 2725512900, ack 232746432, win 8192, options [mss 1440,nop,wscale 8,sackOK,TS val 250809089 ecr 20876893], length 0\\n']\n"
     ]
    }
   ],
   "source": [
    "print lf[0].split()[4]\n",
    "print lt[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = ['19:36', '19:35', '19:37']\n",
    "l.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19:35', '19:36', '19:37']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
