{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specific extraction of information from different formats\n",
    "\n",
    "# This extracts the latency in milliseconds from the ping's result format\n",
    "# 64 bytes from 13.66.225.134: icmp_seq=1 ttl=47 time=36.2 ms => 36.2\n",
    "def get_ping_list(file_name):\n",
    "    l = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                l.append(float(line.split()[6].split(\"=\")[1]))\n",
    "            except:\n",
    "                continue\n",
    "        if len(l) < 1000:\n",
    "            print file_name # to catch any deficiencies\n",
    "        return l\n",
    " \n",
    "# This extracts the latency from files that just have a number per row\n",
    "def get_integer_list(file_name):\n",
    "    l = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                l.append(int(line))\n",
    "            except:\n",
    "                continue\n",
    "    return l\n",
    "\n",
    "\n",
    "# This extracts all latency breakdown results from the giza_latency_raw directory\n",
    "# returns a map {key, [list of latency for the key]}\n",
    "def get_giza_latency_raw_dir_map(file_name):\n",
    "    from collections import defaultdict\n",
    "    d = defaultdict(list)\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            if len(l) == 3:\n",
    "                key = l[0] + l[1]\n",
    "                d[key].append(int(l[2]))\n",
    "            elif len(l) == 4:\n",
    "                key = l[0] + l[1] + l[2]\n",
    "                d[key].append(int(l[3]))\n",
    "            else:\n",
    "                if len(l) == 0 or len(l) == 1:\n",
    "                    continue\n",
    "\n",
    "                d[l[0]].append(int(l[1]))\n",
    "    return d\n",
    "\n",
    "# helper mapping for data extraction within giza_latency_raw directory\n",
    "def get_dc_ip_mappings():\n",
    "    dic = {}\n",
    "    dic['south-central'] = '13.65.92.139'\n",
    "    dic['west1'] = \"13.93.236.162\"\n",
    "    dic['central'] = \"localhost\"\n",
    "    dic['west-central'] = '52.161.28.134'\n",
    "    dic['north-central'] = '157.56.29.194'\n",
    "    dic['east1'] = '191.237.41.69'\n",
    "    dic['east2'] = '13.68.110.92'\n",
    "    dic['us-south-central'] = '13.65.92.139'\n",
    "    dic['us-west1'] = \"13.93.236.162\"\n",
    "    dic['us-central'] = \"localhost\"\n",
    "    dic['asia-japan-east'] = \"13.78.83.9\"\n",
    "    dic['eu-north-europe'] = '52.178.201.184'\n",
    "    dic['eu-uk-west'] = '51.141.11.143'\n",
    "    dic['asia-japan-west'] = '104.214.146.200'\n",
    "    \n",
    "    #     dic['japan-east'] = \"13.78.83.9\"\n",
    "#     dic['north-europe'] = '52.178.201.184'\n",
    "    return dic\n",
    "\n",
    "# helper function to get the configuration information\n",
    "def get_configurations(configuration):\n",
    "    configuration_names = ['2-1-us', '2-1-world', '6-1-us', '6-1-world']\n",
    "    configurations = [[\"central\", \"south-central\", \"west1\"],\n",
    "                      [\"us-central\", \"eu-north-europe\", \"asia-japan-east\"],\n",
    "                      [\"central\", \"west-central\", \"south-central\",\n",
    "                       \"north-central\", \"east1\", \"east2\", \"west1\"],\n",
    "                      [\"us-central\", \"us-south-central\", \"us-west1\",\n",
    "                       \"eu-north-europe\", \"eu-uk-west\", \"asia-japan-east\",\n",
    "                       \"asia-japan-west\"]]\n",
    "    return (configuration_names[configuration], configurations[configuration])\n",
    "\n",
    "\n",
    "# This is used to extract the specific thrift latency from the dc to all other dc's storage within a giza experiment\n",
    "# results are limited to:\n",
    "#   - central to [central, west1, south central]\n",
    "#   - central to [central, japan east, north europe]\n",
    "#   - central to [centra, west central, north central, south central, east1, east2, west1]\n",
    "#   - central to [central, west1, south central, japan east, japan west, north europe, uk west]\n",
    "# input:\n",
    "#    configuration :\n",
    "#      0: us-2-1\n",
    "#      1: world-2-1\n",
    "#      2: us-6-1\n",
    "#      3: world-6-1\n",
    "#    size: 256kb, 1mb, 4mb\n",
    "#    op: put, get\n",
    "def get_thrift_storage_lists(configuration, size, op):\n",
    "    # mapping info\n",
    "    dic = get_dc_ip_mappings()\n",
    "    configuration_name, configurations = get_configurations(configuration)\n",
    "    #configuration_names = ['2-1-us', '2-1-world', '6-1-us', '6-1-world']\n",
    "    #configurations = [[\"central\", \"south-central\", \"west1\"],\n",
    "#                       [\"us-central\", \"eu-north-europe\", \"asia-japan-east\"],\n",
    "#                       [\"central\", \"west-central\", \"south-central\",\n",
    "#                        \"north-central\", \"east1\", \"east2\", \"west1\"],\n",
    "#                       [\"us-central\", \"us-south-central\", \"us-west1\",\n",
    "#                        \"eu-north-europe\", \"eu-uk-west\", \"asia-japan-east\",\n",
    "#                        \"asia-japan-west\"]]\n",
    "    if op == 'get':\n",
    "        thrift_op = \"GET_Thrift_Storage_\"\n",
    "        azure_op = \"Get_Azure_\"\n",
    "    \n",
    "    else:\n",
    "        thrift_op = \"Put_Thrift_Storage_\"\n",
    "        azure_op = \"Put_Azure_\"\n",
    "    thrift_file_name = '/'.join(['giza_latency_raw', configuration_name, size, op, 'giza_trace.log'])\n",
    "    thrift_file_map = get_giza_latency_raw_dir_map(thrift_file_name)\n",
    "    thrift_list = []\n",
    "    azure_list = []\n",
    "    for dc in configurations:\n",
    "        azure_file_name = dc + '_storage_server.log'\n",
    "        azure_file_name = '/'.join(['giza_latency_raw', configuration_name, size, op, azure_file_name])\n",
    "        azure_file_map = get_giza_latency_raw_dir_map(azure_file_name)\n",
    "        if configuration == 0 or configuration == 2:\n",
    "            azure_list.append(azure_file_map[azure_op + 'us-' +  dc])\n",
    "        else:\n",
    "            azure_list.append(azure_file_map[azure_op + dc])\n",
    "        thrift_list.append(thrift_file_map[thrift_op + dic[dc]])\n",
    "    return (thrift_list, azure_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specifically for Data Analysis\n",
    "\n",
    "# get 2d arrar (matrix graph) of the different dc access. Each element is a list of result that can \n",
    "# be summarized\n",
    "# returns a map where key is a tuple\n",
    "def get_2d_data(from_dcs, to_dcs, file_format):\n",
    "    d = {}\n",
    "    for dc1 in from_dcs:\n",
    "        for dc2 in to_dcs:\n",
    "            d[(dc1, dc2)] = get_ping_list(file_format.format(dc1, dc2))\n",
    "    return d\n",
    "\n",
    "# reducer of the results returned by get_2d_data\n",
    "# can be map or vector\n",
    "def do_2d_analysis(data, data_points, reducer, *args):\n",
    "    dict\n",
    "    if type(data) is dict:\n",
    "        d = {}\n",
    "        for key in data:\n",
    "            if len(data[key]) == data_points:\n",
    "                d[key] = reducer(data[key], *args)\n",
    "            else:\n",
    "                d[key] = -1\n",
    "        return d\n",
    "    if type(data) is list:\n",
    "        d = []\n",
    "        for datum in data:\n",
    "            d.append(reducer(datum))\n",
    "        return d\n",
    "\n",
    "# convert any analysis data into a table\n",
    "def make_2d_table(from_dcs, to_dcs, data, file_name):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    table = np.zeros((len(from_dcs), len(to_dcs)), dtype=np.int)\n",
    "    for i in range(len(from_dcs)):\n",
    "        for j in range(len(to_dcs)):\n",
    "            table[i][j] = data[(from_dcs[i], to_dcs[j])]\n",
    "    table = pd.DataFrame(table, index=from_dcs, columns=to_dcs)\n",
    "    table.to_csv(file_name, index=True, header=True, sep=',')\n",
    "\n",
    "# given data points, calculate different percentiles\n",
    "def get_percentiles(data, percentiles):\n",
    "    l = [] \n",
    "    for p in percentiles:\n",
    "        import numpy as np\n",
    "        l.append(np.percentile(data, p))\n",
    "    return l\n",
    "\n",
    "# create bargraph and allows stacking with extra data\n",
    "# input has to be in the form of [[raw_data1], [raw_data2]. etc]\n",
    "def bar_graph_with_error(data1, data2, data_names, x_names, y_label, fname, y_range, no_mean = True):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    f = plt.figure()\n",
    "    ind = np.arange(len(data1))\n",
    "    width = 0.25\n",
    "    if no_mean:\n",
    "        data1_mean = [np.mean(x)/1000 for x in data1]\n",
    "        data2_mean = [np.mean(x)/1000 for x in data2]\n",
    "    \n",
    "#     data1_err = [np.std(x)/1000 for x in data1]    \n",
    "#     data2_err = [np.std(x)/1000 for x in data2]\n",
    "    p1 = plt.bar(ind, data1_mean, width, color='r') #, yerr=data1_err)\n",
    "    if data2 != '':\n",
    "        p2 = plt.bar(ind, data2_mean, width, color='y')#, bottom = data1_mean) #, yerr=data2_err)\n",
    "        plt.legend((p1[0], p2[0]), data_names)\n",
    "    plt.xticks(ind + width/2., x_names)\n",
    "    plt.yticks(np.arange(0, y_range, 50))\n",
    "    plt.ylabel(y_label)\n",
    "#     plt.legend((p1[0], p2[0]), data_names)\n",
    "#     plt.show()\n",
    "    pp = PdfPages(fname + '.pdf')\n",
    "    pp.savefig(f)\n",
    "    plt.clf()\n",
    "    pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ping_latency_raw/west-central-to-north-europe.log\n",
      "ping_latency_raw/north-europe-to-west-central.log\n"
     ]
    }
   ],
   "source": [
    "# Get the ping latencies\n",
    "import numpy as np\n",
    "dcs = ['central', 'west-central', 'north-central', 'east1', 'east2', 'south-central', 'west1', 'west2', 'north-europe', 'uk-west', 'japan-east', 'japan-west']\n",
    "file_format = 'ping_latency_raw/{}-to-{}.log'\n",
    "data = get_2d_data(dcs, dcs, file_format)\n",
    "percentiles = [10, 50, 75, 95]\n",
    "for p in percentiles:\n",
    "    data_summary = do_2d_analysis(data, 1000, np.percentile, p)\n",
    "    make_2d_table(dcs, dcs, data_summary, 'ping_latency_readable/ping_' + str(p) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure_storage_latency_graph/tunneling/2-1-us-put-256kb\n",
      "azure_storage_latency_graph/tunneling/2-1-us-get-256kb\n",
      "azure_storage_latency_graph/tunneling/2-1-us-put-1mb\n",
      "azure_storage_latency_graph/tunneling/2-1-us-get-1mb\n",
      "azure_storage_latency_graph/tunneling/2-1-us-put-4mb\n",
      "azure_storage_latency_graph/tunneling/2-1-us-get-4mb\n",
      "azure_storage_latency_graph/tunneling/6-1-us-put-256kb\n",
      "azure_storage_latency_graph/tunneling/6-1-us-get-256kb\n",
      "azure_storage_latency_graph/tunneling/6-1-us-put-1mb\n",
      "azure_storage_latency_graph/tunneling/6-1-us-get-1mb\n",
      "azure_storage_latency_graph/tunneling/6-1-us-put-4mb\n",
      "azure_storage_latency_graph/tunneling/6-1-us-get-4mb\n",
      "azure_storage_latency_graph/tunneling/2-1-world-put-256kb\n",
      "azure_storage_latency_graph/tunneling/2-1-world-get-256kb\n",
      "azure_storage_latency_graph/tunneling/2-1-world-put-1mb\n",
      "azure_storage_latency_graph/tunneling/2-1-world-get-1mb\n",
      "azure_storage_latency_graph/tunneling/2-1-world-put-4mb\n",
      "azure_storage_latency_graph/tunneling/2-1-world-get-4mb\n",
      "azure_storage_latency_graph/tunneling/6-1-world-put-256kb\n",
      "azure_storage_latency_graph/tunneling/6-1-world-get-256kb\n",
      "azure_storage_latency_graph/tunneling/6-1-world-put-1mb\n",
      "azure_storage_latency_graph/tunneling/6-1-world-get-1mb\n",
      "azure_storage_latency_graph/tunneling/6-1-world-put-4mb\n",
      "azure_storage_latency_graph/tunneling/6-1-world-get-4mb\n"
     ]
    }
   ],
   "source": [
    "# Get the tunneling latency\n",
    "import numpy as np\n",
    "from_dcs = ['central']\n",
    "configurations = [0, 2]\n",
    "sizes = ['256kb', '1mb', '4mb']\n",
    "ops = ['put', 'get']\n",
    "cur_dir = 'azure_storage_latency_graph/tunneling'\n",
    "for configuration in configurations:\n",
    "    for size in sizes:\n",
    "        for op in ops:\n",
    "            config, to_dcs = get_configurations(configuration)\n",
    "            thrift_latency, azure_latency = get_thrift_storage_lists(configuration, size, op)\n",
    "#             transfer_latency = []\n",
    "#             for i in range(len(thrift_latency)):\n",
    "#                 transfer_latency.append(np.subtract(thrift_latency[i], azure_latency[i]))\n",
    "            bar_graph_with_error(thrift_latency, azure_latency, ('Transfer', 'Azure'), to_dcs, 'Latency (ms)', '{}/{}-{}-{}'.format(cur_dir, config, op, size), 301)\n",
    "configurations = [1, 3]\n",
    "sizes = ['256kb', '1mb', '4mb']\n",
    "ops = ['put', 'get']\n",
    "cur_dir = 'azure_storage_latency_graph/tunneling'\n",
    "for configuration in configurations:\n",
    "    for size in sizes:\n",
    "        for op in ops:\n",
    "            config, to_dcs = get_configurations(configuration)\n",
    "            thrift_latency, azure_latency = get_thrift_storage_lists(configuration, size, op)\n",
    "#             transfer_latency = []\n",
    "#             for i in range(len(thrift_latency)):\n",
    "#                 transfer_latency.append(np.subtract(thrift_latency[i], azure_latency[i]))\n",
    "            bar_graph_with_error(thrift_latency, azure_latency, ('Transfer', 'Azure'), to_dcs, 'Latency (ms)', '{}/{}-{}-{}'.format(cur_dir, config, op, size), 401)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the Giza Put and Get Latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
